\section{Syntactic Diff}
\label{sec:syndiff}

\sd~is a general text analysis framework for transforming (modifying) text with
respect to a reference corpus using various edits; the goal is to transform a
text object into another so as to better match the reference corpus. Aside from
modifying single sentences, it can also be used to make syntactic comparisons
between two bodies of text as well as using edits performed on a collection of
sentences as features for text representation. We hope to be able to transform,
compare, summarize, and induce features from text. The proposed definition of
\sd~will give us the power and flexibility to solve these proposed tasks.

\subsection{Reference Language Models (LMs)}

The reference corpus provides guidance for how we transform a given text object
and enables flexible customization of the perspective for defining
transformations in \sd. The choice of the reference corpus is thus intentionally
application specific.

Given a reference corpus, our goal is to find transformations that can convert
any given text object into one that matches the reference corpus as well as
possible. Specifically, we would seek transformations to convert the original
text object into a new one that would have a higher probability according to the
reference language model (LM).

Without loss of generality, we make use of an $n$-gram LM\@. An $n$-gram LM
assigns probability to a sequence of $m$ words, where each word is conditioned
on the previous $n-1$ words. Thus, for LM $\theta$:
$$P_{\theta}(w_1,w_2,\ldots,w_m) \approx
    \prod_{i=1}^m P(w_i|w_{i-n+1},\ldots,w_{i-1}).$$
In practice, we reserve probability mass for unseen events by smoothing our
LM\@. A simple form of smoothing used by the~\sd~LM is linear
interpolation~\cite{j-and-m}.

\ignore{ An example of this smoothing for a 3-gram language model
is
\begin{equation*}
    \begin{split}
        P_{\theta}(w_i|w_{i-2},w_{i-1}) &= \lambda_3 P(w_i|w_{i-2},w_{i-1}) \\
        &+ \lambda_2 P(w_i|w_{i-1}) \\
        &+ \lambda_1 P(w_i), \\
    \end{split}
\end{equation*}
where $\lambda_1 + \lambda_2 + \lambda_3 = 1$ in order to ensure a valid
probability distribution.}

Perplexity is a measure for LM evaluation. It can be used to test the likelihood
of a sequence given an LM $\theta$.
$$Perp(w_1,w_2,\ldots,w_m) =
    \left(\prod_{i=1}^m
    \frac{1}{P_{\theta}(w_i|w_{i-n+1},\ldots,w_{i-1}}\right)^{\frac{1}{n}}$$
A lower perplexity (or cross-entropy) means that the sequence was more likely to
have been generated by $\theta$. We use perplexity per word as a normalized form
of scoring for candidate sentences in~\sd. For a more rigorous and detailed
introduction to LMs and their related concepts, please consult Jurafsky and
Martin~\cite{j-and-m}.

\subsection{Transformation Edits}

We define three basic edit operations on sentences:
\begin{enumerate}
    \item \textbf{Insert} the word $w$ after position $j$ in sentence $S$:
        $insert(S, j, w)$. The inserted word is drawn from a set of words
        $V^{INS}$.
    \item \textbf{Remove} the word at position $j$ in $S$: $remove(S, j)$.
    \item \textbf{Substitute} the word at position $j$ in $S$ with $w$:
        $substitute(S, j, w)$. The substituted word is drawn from a set of words
        potentially dependent on $w_j$: $V^{SUB}(w_j)$.
\end{enumerate}

These three edit functions are used to incrementally transform the original
sentence into multiple candidate sentences. The candidate sentences are scored
based on perplexity using the reference language model, and the sentence with
the lowest perplexity per word becomes the output. Setting $V^{INS}$ to only
insert non-content words and setting $V^{SUB}(w)$ to replace words with similar
words or inflected forms of the word allows \texttt{insert} and \texttt{delete}
to preserve the original meaning of the sentence, though this is not a
requirement. It's possible that the two sets are defined to capture some other
grammatical meaning as a particular task demands.

$V^{INS}$ and $V^{SUB}(w)$ may be chosen arbitrarily, thus making size a
variable of consideration. In the case where \sd~is used at large scale on big
data, these sets may be reduced to only the most promising elements, in effect
reducing the (albeit already competitive) runtime.

For an index $j$, there are candidates generated from each edit function for a
total of $|V^{INS}| + 1 + |V^{SUB}(w_j)|$ edits in addition to the original
sentence, which is also regarded as a candidate. Each iteration of \sd~only
performs the edit functions on one index. The index $j$ is chosen by the least
likely $n$-gram from the current sentence $S=w_1,w_2,\ldots,w_m$ (which is most
promising for increasing the likelihood and lowering the perplexity). The index
of this $n$-gram is given by
$$j = \argmax_{i\in [0,m]} \left\{Perp(w_i,w_{i+1},\ldots,w_{i+n-1})\right\}.$$

\begin{algorithm}[t]
\caption{The~\sd~algorithm}
\begin{algorithmic}
    \Procedure{SyntacticDiff}{$S$}
    \State $candidates$ $\leftarrow \{\}$
    \State Initialize $V^{INS}$
    \State Initialize $V^{SUB}(w)\forall w\in V$
    \State\sd($S$, 0)
    \State \Return best candidate from $candidates$
    \EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{The recursive~\sd~algorithm}
\begin{algorithmic}
    \Procedure{SyntacticDiff}{$S$, $depth$}
    \State \Return \textbf{if} $depth = k$
    \State $j \leftarrow \argmax_{i\in [0,m]}
        \left\{Perp(w_i,w_{i+1},\ldots,w_{i+n-1}\in S)\right\}$
    \For{$w\in V^{INS}$}
    \State $S' \leftarrow insert(S, j, w)$
    \State $candidates.add(S')$
    \State\sd($S', depth+1$)
    \EndFor
    \State $S' \leftarrow remove(S, j)$
    \State $candidates.add(S')$
    \State\sd($S', depth+1$)
    \For{$w\in V^{SUB}(w_j)$}
    \State $S' \leftarrow substitute(S, j, w)$
    \State $candidates.add(S')$
    \State\sd($S', depth+1$)
    \EndFor
    \EndProcedure
\end{algorithmic}
\end{algorithm}


Next, we need to choose $k$, the number of iterations to perform. Each iteration
operates on all candidate sentences, so for iteration one, only one sentence is
operated on. In the second iteration, all new candidates are operated on.
Generally, we choose $k\in[1,5]$ in order to preserve the main content of the
original sentence. The full algorithm for \sd~is given in Alg. 1 and Alg. 2.
Initially, we learn an $n$-gram language model $\theta$ from a reference corpus
and pick a maximum depth $k$. Not shown in the pseudocode are checks to ensure
edits aren't recomputed for duplicated sentences, since the same candidate
sentence may be generated in different branches of the algorithm. This is a
simple dynamic programming optimization.

\subsection{Weighted Edits}
\label{sec:weighted}

Until now, each candidate sentence is scored equally based on minimizing
perplexity per word, regardless of the number or type of edits. This gives the
simple scoring function
$$S^*=\argmin_{S\in candidates}\left\{Perp(S)\right\}.$$
However, we can improve the scoring function to capture some meaning in each
edit:
$$S^*=\argmin_{S\in candidates}\left\{\alpha\cdot
Perp(S)+(1-\alpha)\cdot W_S\right\},$$
where $W_S$ is the edit weight (or edit penalty) of $S$ and $\alpha\in[0,1]$.
$\alpha$ controls the tradeoff between lowering perplexity and lowering penalty;
for simplicity, in this first study of \sd, we simply set $\alpha=0.5$ in our
experiments, though obviously it is also interesting to further study how to
optimize $\alpha$ in the future work. The edit penalty of $S$ can be determined
as the average penalty over all edits performed on $S$. Each penalty edit weight
can be on $[0,1]$.

In this paper, we define four penalties, though the framework is general and any
penalty type may be defined using information from the current sentence or
reference corpus. We define: an \texttt{insert}, \texttt{remove}, and
\texttt{substitute} penalty. We also have a base penalty incurred if \emph{any}
edit is performed, penalizing sentences with many edits.

If we set all penalties to zero, we arrive at the original \sd~formulation;
thus, weighted \sd~is a generalization of the previous description. Furthermore,
these penalties can be further refined to vary according to the specific words
inserted, deleted, or substituted, and optimized based on specific needs of an
application. Since only the scoring function to find $S^*$ changes for weighted
edits, the \sd~algorithm remains unchanged from Alg. 1 and Alg. 2.
